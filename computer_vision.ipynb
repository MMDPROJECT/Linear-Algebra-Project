{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232e43b2",
   "metadata": {},
   "source": [
    "![](./sbu_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c765e1c-99bd-470e-8a65-9c5ffaaacc6c",
   "metadata": {},
   "source": [
    "# Image Proccessing Related Project\n",
    "- **This Project is done to show some of the applications of the Linear Algebra in the future lesson**\n",
    "### Written and Developed By: Mohammad Hossein Basouli, SID: 401222020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52911915-8fa8-47c2-a1cc-872d5604a862",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae59ba8-0001-4dfc-8325-94b35caa8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from numpy import sin, cos, radians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a048d6-fdc1-4cf2-9582-752bad298a0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Image Manipulation Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9654f686-4199-4868-b8f3-0de4716716da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageManip:\n",
    "    \n",
    "    @staticmethod\n",
    "    def resize_img(img: np.ndarray, x_scale, y_scale):\n",
    "        h, w, channels = img.shape # getting dimensions of the image\n",
    "        new_height, new_width = int(h * y_scale), int(w * x_scale) # calculating the output dimensions\n",
    "        y_scale_inv, x_scale_inv = h / new_height, w / new_width # calculating the inverse scale\n",
    "        new_img = np.zeros((new_height, new_width, channels), dtype = np.uint8) # creating a new matrix correspoding to the new dimensions\n",
    "        \n",
    "        for i in range(new_height):\n",
    "            for j in range(new_width):\n",
    "                org_heigth = int(y_scale_inv * i) # original height of i\n",
    "                org_width = int(x_scale_inv * j) # original width of j\n",
    "                new_img[i, j] = img[org_heigth, org_width] # reflexing an invertval of pixels to the new image\n",
    "        return new_img\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def rotate_img(img: np.ndarray, theta):\n",
    "        theta = radians(theta) # converting the angle to radian\n",
    "        h, w, channels = img.shape # getting dimensions of the image\n",
    "        center_i, center_j = h // 2, w // 2 # calculating the center of the image\n",
    "        new_img = np.zeros((h, w, channels), dtype = np.uint8) # creating a new matrix correspoding to scales\n",
    "        # Definning Rotation Matrix in 2-D\n",
    "        rotation_matrix = np.array([[cos(theta), -sin(theta)], \n",
    "                                    [sin(theta), cos(theta)]])\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                y, x = i - center_i, j - center_j # calculating the corresponding vector from center\n",
    "                new_j, new_i = np.matmul(rotation_matrix, [x, y]) # final vector calculated after rotation\n",
    "                new_j, new_i = int(new_j + center_j), int(new_i + center_i)\n",
    "                if 0 <= new_j < w - 1 and 0 <= new_i < h - 1: # checking if the final pixel doesn't exceed the screen\n",
    "                    new_img[i, j] = img[new_i, new_j] # reflexing each pixel of the old img\n",
    "        return new_img\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def blur_img(img: np.ndarray, method: int):\n",
    "       h, w, channels = img.shape # shape of the image\n",
    "       new_img = np.zeros((h, w, channels), dtype = np.uint8) # creating a new matrix corresponding to scales\n",
    "        \n",
    "       for i in range(0, h, 10):\n",
    "           for j in range(0, w, 10):\n",
    "               mat = img[i : i + 10, j : j + 10] # a 10 by 10 part of the image to get convolved\n",
    "               \n",
    "               if method == 1:\n",
    "                   # Replacing the output \n",
    "                   new_img[i : i + 10, j : j + 10] = ImageManip.convolve_gaussian_kernel_1(mat) \n",
    "               elif method == 2:\n",
    "                   # Replacing the output \n",
    "                   new_img[i : i + 10, j : j + 10] = ImageManip.convolve_gaussian_kernel_2(mat)\n",
    "       return new_img\n",
    "        \n",
    "            \n",
    "    @staticmethod\n",
    "    def convolve_gaussian_kernel_1(mat: np.ndarray): # First Method to Sharpen an Image\n",
    "        h, w, channels = mat.shape # getting dimensions of the image\n",
    "        new_mat = np.zeros((h, w, channels), dtype = np.uint8) # creating a new matrix correspoding to scales\n",
    "        \n",
    "        center_i, center_j = h // 2, w // 2\n",
    "        \n",
    "        B_channel = mat[::, ::, 0] # all Blue Channels\n",
    "        G_channel = mat[::, ::, 1] # all Green Channels\n",
    "        R_channel = mat[::, ::, 2] # all Red Channels\n",
    "        \n",
    "        B_average = np.sum(B_channel, axis= None) // (h * w) # average of B-channels\n",
    "        G_average = np.sum(G_channel, axis= None) // (h * w) # average of G-channels\n",
    "        R_average = np.sum(R_channel, axis= None) // (h * w) # average of R-channels\n",
    "        \n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                d = int((((i - center_i)/center_i) ** 2 + ((j - center_j)/center_j) ** 2) ** 0.5) # a ratio to get subtracted from average\n",
    "                new_mat[i, j] = np.array([B_average - d * B_average, G_average - d * G_average, R_average - d * R_average]) # average - ratio * average\n",
    "        return new_mat\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def convolve_gaussian_kernel_2(mat: np.ndarray): # Second Method to Sharpen an Image\n",
    "        h, w, channels = mat.shape # getting dimensions of the image\n",
    "        new_mat = np.zeros((h, w, channels), dtype = np.uint8) # creating a new matrix correspoding to scales\n",
    "        \n",
    "        center_i, center_j = h // 2, w // 2\n",
    "        \n",
    "        B_channel = mat[::, ::, 0] # all Blue Channels\n",
    "        G_channel = mat[::, ::, 1] # all Green Channels\n",
    "        R_channel = mat[::, ::, 2] # all Red Channels\n",
    "        \n",
    "        B_average = np.sum(B_channel, axis= None) // (h * w) # average of B-channels\n",
    "        G_average = np.sum(G_channel, axis= None) // (h * w) # average of G-channels\n",
    "        R_average = np.sum(R_channel, axis= None) // (h * w) # average of R-channels\n",
    "        \n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                new_mat[i, j] = np.array([B_average, G_average, R_average]) # replacing each image by the average color channel of it's region\n",
    "        return new_mat\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def sharpen_img(img: np.ndarray):\n",
    "       h, w, channels = img.shape # shape of the image\n",
    "       new_img = np.zeros((h - 2, w - 2, channels), dtype = np.uint8) # creating a new matrix corresponding to scales\n",
    "        \n",
    "       for i in range(1, h - 1):\n",
    "           for j in range(1, w - 1): \n",
    "               mat = img[i - 1 : i + 2, j - 1 : j + 2] # part of image to get convolved\n",
    "               new_img[i - 1, j - 1] = ImageManip.convolve_sharpen_matrix(mat) # placing in the output img\n",
    "       return new_img\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def convolve_sharpen_matrix(mat: np.ndarray):\n",
    "        # Specific Kernel to Sharpen the Image\n",
    "        sharpen_kernel = np.array([[0, -1, 0], \n",
    "                                   [-1, 5, -1], \n",
    "                                   [0, -1, 0]])\n",
    "        \n",
    "        accumulator_b = 0 # acumulator for blue channel\n",
    "        accumulator_g = 0 # acumulator for green channel\n",
    "        accumulator_r = 0 # acumulator for red channel\n",
    "        \n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                # Convolution with Sharpenning Kernel\n",
    "                accumulator_b += sharpen_kernel[i, j] * mat[i, j, 0] \n",
    "                accumulator_g += sharpen_kernel[i, j] * mat[i, j, 1] \n",
    "                accumulator_r += sharpen_kernel[i, j] * mat[i, j, 2] \n",
    "\n",
    "        return np.clip([accumulator_b, accumulator_g, accumulator_r], 0, 255) # limiting the output\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def edge_detect_img(img: np.ndarray): \n",
    "       h, w, channels = img.shape # shape of the image\n",
    "       new_img = np.zeros((h - 2, w - 2, channels), dtype = np.uint8) # creating output img (h - 2, w - 2 due to convolution)\n",
    "            \n",
    "       for i in range(1, h - 1):\n",
    "           for j in range(1, w - 1): \n",
    "               mat = img[i - 1 : i + 2, j - 1 : j + 2] # part of image to get convolved\n",
    "               new_img[i - 1, j - 1] = ImageManip.convolve_sobel_kernel(mat) # placing in the output img\n",
    "       return new_img\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def convolve_sobel_kernel(mat: np.ndarray): \n",
    "        # Specific Kernels for Detecting Edges\n",
    "        sobel_kernel_vertical = np.array([[1, 0, -1], \n",
    "                                          [1, 0, -1], \n",
    "                                          [1, 0, -1]])\n",
    "        \n",
    "        sobel_kernel_horizontal = np.array([[1, 1, 1], \n",
    "                                            [0, 0, 0], \n",
    "                                            [-1, -1, -1]])\n",
    "        \n",
    "        sobel_kernel_diag_45_pos = np.array([[1, 1, 0], \n",
    "                                             [1, 0, -1], \n",
    "                                             [0, -1, -1]])\n",
    "        \n",
    "        sobel_kernel_diag_45_neg = np.array([[0, 1, 1], \n",
    "                                             [-1, 0, 1], \n",
    "                                             [-1, -1, 0]])\n",
    "        \n",
    "        accumulator_b_v = 0 # acumulator for blue channel of vertical lines\n",
    "        accumulator_g_v = 0 # acumulator for green channel of vertical lines\n",
    "        accumulator_r_v = 0 # acumulator for red channel of vertical lines\n",
    "        \n",
    "        accumulator_b_h = 0 # acumulator for blue channel of horizontal lines\n",
    "        accumulator_g_h = 0 # acumulator for green channel of horizontal lines\n",
    "        accumulator_r_h = 0 # acumulator for red channel of horizontal lines\n",
    "\n",
    "        accumulator_b_diag_45_pos = 0 # acumulator for blue channel of diagonal lines with slope 1\n",
    "        accumulator_g_diag_45_pos = 0 # acumulator for green channel of diagonal lines with slope 1\n",
    "        accumulator_r_diag_45_pos = 0 # acumulator for red channel of diagonal lines with slope 1\n",
    "        \n",
    "        accumulator_b_diag_45_neg = 0 # acumulator for blue channel of diagonal lines with slope -1\n",
    "        accumulator_g_diag_45_neg = 0 # acumulator for green channel of diagonal lines with slope -1\n",
    "        accumulator_r_diag_45_neg = 0 # acumulator for red channel of diagonal lines with slope -1\n",
    "\n",
    "        \n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                # Convolving with Vertical Kernel\n",
    "                accumulator_b_v += sobel_kernel_vertical[i, j] * mat[i, j, 0] \n",
    "                accumulator_g_v += sobel_kernel_vertical[i, j] * mat[i, j, 1] \n",
    "                accumulator_r_v += sobel_kernel_vertical[i, j] * mat[i, j, 2] \n",
    "                # Convolving with Horizontal Kernel\n",
    "                accumulator_b_h += sobel_kernel_horizontal[i, j] * mat[i, j, 0] \n",
    "                accumulator_g_h += sobel_kernel_horizontal[i, j] * mat[i, j, 1] \n",
    "                accumulator_r_h += sobel_kernel_horizontal[i, j] * mat[i, j, 2] \n",
    "                # Convolving with diagonal positive 45 degree Kernel\n",
    "                accumulator_b_diag_45_pos += sobel_kernel_diag_45_pos[i, j] * mat[i, j, 0] \n",
    "                accumulator_g_diag_45_pos += sobel_kernel_diag_45_pos[i, j] * mat[i, j, 1]\n",
    "                accumulator_r_diag_45_pos += sobel_kernel_diag_45_pos[i, j] * mat[i, j, 2] \n",
    "                # Convolving with diagonal negative 45 degree Kernel\n",
    "                accumulator_b_diag_45_neg += sobel_kernel_diag_45_neg[i, j] * mat[i, j, 0] \n",
    "                accumulator_g_diag_45_neg += sobel_kernel_diag_45_neg[i, j] * mat[i, j, 1]\n",
    "                accumulator_r_diag_45_neg += sobel_kernel_diag_45_neg[i, j] * mat[i, j, 2] \n",
    "                \n",
    "\n",
    "        accumulator_b_final = int((accumulator_b_v ** 2 + accumulator_b_h ** 2 + accumulator_b_diag_45_pos ** 2 + accumulator_b_diag_45_neg ** 2) ** 0.5) # taking sqrt of vertical and horizonal squares\n",
    "        accumulator_g_final = int((accumulator_g_v ** 2 + accumulator_g_h ** 2 + accumulator_g_diag_45_pos ** 2 + accumulator_g_diag_45_neg ** 2) ** 0.5) # taking sqrt of vertical and horizonal squares\n",
    "        accumulator_r_final = int((accumulator_r_v ** 2 + accumulator_r_h ** 2 + accumulator_r_diag_45_pos ** 2 + accumulator_r_diag_45_neg ** 2) ** 0.5) # taking sqrt of vertical and horizonal squares\n",
    "\n",
    "        return np.clip([accumulator_b_final, accumulator_g_final, accumulator_r_final], 0, 255) # limiting the output\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def gray_scale_convertion(img: np.ndarray):\n",
    "        h, w, channels = img.shape # shape of the image\n",
    "        new_img = np.zeros((h, w), dtype = np.single) # creating a new matrix corresponding to scales\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                # Calcutating C_srgb\n",
    "                B_srgb = img[i, j, 0] / 255\n",
    "                G_srgb = img[i, j, 1] / 255\n",
    "                R_srgb = img[i, j, 2] / 255\n",
    "\n",
    "                # Calculating C_linear\n",
    "                B_linear = None\n",
    "                G_linear = None\n",
    "                R_linear = None\n",
    "                \n",
    "                if B_srgb <= 0.04045: B_linear = B_srgb / 12.92\n",
    "                else: B_linear = ((B_srgb + 0.055) / 1.055) ** 2.4\n",
    "\n",
    "                if G_srgb <= 0.04045: G_linear = G_srgb / 12.92\n",
    "                else: G_linear = ((G_srgb + 0.055) / 1.055) ** 2.4\n",
    "\n",
    "                if R_srgb <= 0.04045: R_linear = R_srgb / 12.92\n",
    "                else: R_linear = ((R_srgb + 0.055) / 1.055) ** 2.4\n",
    "\n",
    "                # Calculating Y_linear by specific coeficients\n",
    "                Y_linear = 0.2126 * R_linear + 0.7152 * G_linear + 0.0722 * B_linear\n",
    "                new_img[i, j] = Y_linear\n",
    "        return new_img\n",
    "\n",
    "    @staticmethod\n",
    "    def color_inversion(img: np.ndarray):\n",
    "        h, w, channels = img.shape # shape of the image\n",
    "        new_img = np.zeros((h, w, channels), dtype = np.uint8) # creating a new matrix corresponding to scales\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                inverted_b = 255 - img[i, j, 0]\n",
    "                inverted_g = 255 - img[i, j, 1]\n",
    "                inverted_r = 255 - img[i, j, 2]\n",
    "                new_img[i, j] = np.array([inverted_b, inverted_g, inverted_r])\n",
    "        return new_img\n",
    "\n",
    "    @staticmethod\n",
    "    def color_balance(img: np.ndarray): \n",
    "        h, w, channels = img.shape # shape of the image\n",
    "        \n",
    "        blue = img[:, :, 0]\n",
    "        green = img[:, :, 1]\n",
    "        red = img[:, :, 2]\n",
    "\n",
    "        # Normalize the RGB channels\n",
    "        red = (red - red.min()) / (red.max() - red.min())\n",
    "        green = (green - green.min()) / (green.max() - green.min())\n",
    "        blue = (blue - blue.min()) / (blue.max() - blue.min())\n",
    "\n",
    "        # Combine the normalized RGB channels\n",
    "        new_img = np.dstack((red, green, blue))\n",
    "\n",
    "        # Scale the pixel values back to the range [0, 255]\n",
    "        new_img = (new_img * 255).astype(np.uint8)\n",
    "        return new_img\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190cc2c6-27fb-42fb-ad0d-f99629b9ec62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Image Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deda6e3d-4475-4efa-a75c-07f1237594e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"kuroky.jpg\")\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "new_img = ImageManip.resize_img(img, 1.5, 1.5)\n",
    "cv2.imshow(\"Resized Image\", new_img)\n",
    "cv2.imwrite(\"resized_kuroky_by_1.5.png\", new_img)\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6376c5-f193-4253-bcf9-a79dc53f0a0a",
   "metadata": {},
   "source": [
    "![resized Image](./resized_kuroky_by_1.5.png) ![original image](./kuroky.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d54648-a70a-4724-8787-43bdbd8f24e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Image Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c0a81f4-bd32-48fd-af99-4c40b43db0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = ImageManip.rotate_img(img, 45)\n",
    "cv2.imshow(\"Rotated Image\", new_img)\n",
    "cv2.imwrite(\"rotated_kuroky_by_45_deg.png\", new_img)\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be85dbdb-ad16-42e4-bbe4-4671a5d1edc4",
   "metadata": {},
   "source": [
    "![rotated image by 45 degres](rotated_kuroky_by_45_deg.png) ![original image](kuroky.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761609a-c767-4c93-83ff-223fe488239c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Image Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60722e9a-b672-43a0-87ad-4754df4286b5",
   "metadata": {},
   "source": [
    "## Image Blurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed86188c-882b-43f1-adcb-1a98d0a06809",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img_1 = ImageManip.blur_img(img, 1)\n",
    "cv2.imshow(\"Blurred Image by method 1\", new_img_1)\n",
    "new_img_2 = ImageManip.blur_img(img, 2)\n",
    "cv2.imshow(\"Blurred Image by method 2\", new_img_2)\n",
    "cv2.imwrite(\"blurred_kuroky_1.png\", new_img_1)\n",
    "cv2.imwrite(\"blurred_kuroky_2.png\", new_img_2)\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b28aa-092e-486f-97a7-d0c8a196f5fe",
   "metadata": {},
   "source": [
    "![blurred image](blurred_kuroky_1.png) ![blurred image](blurred_kuroky_2.png) ![original image](kuroky.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4373c71f-8610-4b3e-ac20-dc62cdc2dd33",
   "metadata": {},
   "source": [
    "## Sharpening an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e3de85-01e0-4eb4-935b-cb6c8ff6c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = ImageManip.sharpen_img(img)\n",
    "cv2.imshow(\"Sharpened Image\", new_img)\n",
    "cv2.imwrite(\"sharpened_kuroky.png\", new_img)\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384dc60-8d04-4779-821e-0ec01d80e338",
   "metadata": {},
   "source": [
    "![sharpened image](sharpened_kuroky.png) ![original image](kuroky.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088635f6-0076-405e-92f9-5ff476423895",
   "metadata": {},
   "source": [
    "## Edge Detection on an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6d6e6-aab0-4e85-9c17-c2d0ab3beafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"machine.png\")\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "new_img = ImageManip.edge_detect_img(img)\n",
    "cv2.imshow(\"Edges of the Image\", new_img)\n",
    "cv2.imwrite(\"edges_machine.png\", new_img)\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa889739-9fb2-4241-a047-21a332421fa9",
   "metadata": {},
   "source": [
    "![image after edge detection](edges_machine.png) ![original image](machine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f1198-d23a-4f76-a623-395db2d654dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Color Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8120b7-44e9-4227-ba71-72ec85ab04e1",
   "metadata": {},
   "source": [
    "## Gray Scale Convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e357c9-c97b-4ef8-b754-1e9bc3f4693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"machine.png\")\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "new_img = ImageManip.gray_scale_convertion(img)\n",
    "gray_image = cv2.cvtColor(new_img, cv2.COLOR_GRAY2BGR)\n",
    "cv2.imshow(\"Gray Scale of Image\", gray_image)\n",
    "result = cv2.normalize(gray_image, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "cv2.imwrite(\"gray_scale_machine.jpg\", result)\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a5c18-0c3e-4cfc-8692-6cc1e11f888e",
   "metadata": {},
   "source": [
    "![gray scale of image](gray_scale_machine.jpg) ![original image](machine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38769462-d11f-4706-bfe7-5be41d1f2b98",
   "metadata": {},
   "source": [
    "## Color Inversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c966d9-8547-4580-894f-2e1344666904",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"machine.png\")\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "new_img = ImageManip.color_inversion(img)\n",
    "cv2.imshow(\"Color Inversion\", new_img)\n",
    "cv2.imwrite(\"color_inversion_machine.png\", new_img)\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a13f5-a42f-45cb-93dd-dea332db3bba",
   "metadata": {},
   "source": [
    "![color inversion](color_inversion_machine.png) ![original image](machine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602aefd-fc93-436a-9bd9-c0ddba3cf157",
   "metadata": {},
   "source": [
    "## Color Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970afbd-344d-4154-bc21-199e2ae0eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"hillside.png\")\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "new_img = ImageManip.color_balance(img)\n",
    "cv2.imshow(\"Color Balanced\", new_img)\n",
    "cv2.imwrite(\"color_balance_hillside.png\", new_img)\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad1444-e512-4f81-9e5a-cc94661aee24",
   "metadata": {},
   "source": [
    "![color balanced](color_balance_hillside.png) ![original image](hillside.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bffc607-1d1a-4d4d-9984-1cc2e3badfb3",
   "metadata": {},
   "source": [
    "# Extra:\n",
    "- 2 non-trivial approaches to blur image (using gaussian kernel)\n",
    "- considering 4 different cases for a line to get detected\n",
    "- non-trivial approach for gray-scaling\n",
    "- non-trivial approach for color balancing\n",
    "- efficient approaches used for all transformations as you can see in each part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
